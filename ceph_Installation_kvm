Case1:// ceph Installation:

[root@kvmnode ~]# virsh list --all
 Id   Name            State
--------------------------------
 -    cephcluster01   shut off
 -    cephcluster02   shut off
 -    cephcluster03   shut off

[root@kvmnode ~]# virsh list --all
 Id   Name            State
--------------------------------
 -    cephcluster01   shut off
 -    cephcluster02   shut off
 -    cephcluster03   shut off

[root@kvmnode ~]# virsh start cephcluster01
Domain 'cephcluster01' started

[root@kvmnode ~]# virsh start cephcluster02
Domain 'cephcluster02' started

[root@kvmnode ~]# virsh start cephcluster03
Domain 'cephcluster03' started

[root@kvmnode ~]# virsh list --all
 Id   Name            State
-------------------------------
 1    cephcluster01   running
 2    cephcluster02   running
 3    cephcluster03   running


[root@kvmnode ~]# ssh-keygen
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /root/.ssh/id_rsa
Your public key has been saved in /root/.ssh/id_rsa.pub
The key fingerprint is:
SHA256:zxza8JoM6g13eSt24XyOl/FLTWuSHSedT/pS3l+sHmc root@kvmnode
The key's randomart image is:
+---[RSA 3072]----+
|                 |
|                 |
|                 |
|               ..|
|        S .   .o=|
|         X... oB*|
|    . o +oB. *+*E|
|     = +o++o+.=*+|
|   .o ..+oo+..+o+|
+----[SHA256]-----+
[root@kvmnode ~]# ls -la .ssh/
id_rsa           id_rsa.pub       known_hosts      known_hosts.old
[root@kvmnode ~]# ls -la .ssh/id_rsa.pub

[root@kvmnode ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@cephcluster01
[root@kvmnode ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@cephcluster02
[root@kvmnode ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@cephcluster03


#subscription-manager register --username=XXXXXX --password=XXXXXX-- in all nodes 


[root@cephcluster01 ~]# subscription-manager register --username=XXXXXX --password=XXXXXX
This system is already registered. Use --force to override
[root@cephcluster01 ~]# exit
logout
Connection to cephcluster01 closed.
[root@kvmnode ~]# ssh cephcluster02
Last login: Tue Mar 25 19:42:39 2025 from 192.168.29.23
[root@cephcluster02 ~]# subscription-manager register --username=XXXXXX --password=XXXXXX
This system is already registered. Use --force to override
[root@cephcluster02 ~]# exit
logout
Connection to cephcluster02 closed.
[root@kvmnode ~]# ssh cephcluster03
Last login: Tue Mar 25 20:01:38 2025 from 192.168.29.23
[root@cephcluster03 ~]# subscription-manager register --username=XXXXXX --password=XXXXXX
This system is already registered. Use --force to override
[root@cephcluster03 ~]# exit
logout
Connection to cephcluster03 closed.
[root@kvmnode ~]#

NOTE :- Register all machines ( Example for Node01, Node01 & Node03 )

subscription-manager repos --enable=rhceph-5-tools-for-rhel-9-x86_64-rpms --enable=rhel-9-for-x86_64-baseos-rpms  --enable=rhel-9-for-x86_64-appstream-rpms


[root@cephcluster01 ~]# subscription-manager repos --enable=rhceph-5-tools-for-rhel-9-x86_64-rpms --enable=rhel-9-for-x86_64-baseos-rpms  --enable=rhel-9-for-x86_64-appstream-rpms
Repository 'rhceph-5-tools-for-rhel-9-x86_64-rpms' is enabled for this system.
Repository 'rhel-9-for-x86_64-baseos-rpms' is enabled for this system.
Repository 'rhel-9-for-x86_64-appstream-rpms' is enabled for this system.
[root@cephcluster01 ~]# exit
logout
Connection to cephcluster01 closed.
[root@kvmnode ~]# ssh cephcluster02
Last login: Tue Mar 25 20:01:55 2025 from 192.168.29.23
[root@cephcluster02 ~]# subscription-manager repos --enable=rhceph-5-tools-for-rhel-9-x86_64-rpms --enable=rhel-9-for-x86_64-baseos-rpms  --enable=rhel-9-for-x86_64-appstream-rpms
Repository 'rhceph-5-tools-for-rhel-9-x86_64-rpms' is enabled for this system.
Repository 'rhel-9-for-x86_64-baseos-rpms' is enabled for this system.
Repository 'rhel-9-for-x86_64-appstream-rpms' is enabled for this system.
[root@cephcluster02 ~]# exit
logout
Connection to cephcluster02 closed.
[root@kvmnode ~]# ssh cephcluster03
Last login: Tue Mar 25 20:05:56 2025 from 192.168.29.23
[root@cephcluster03 ~]# subscription-manager repos --enable=rhceph-5-tools-for-rhel-9-x86_64-rpms --enable=rhel-9-for-x86_64-baseos-rpms  --enable=rhel-9-for-x86_64-appstream-rpms
Repository 'rhceph-5-tools-for-rhel-9-x86_64-rpms' is enabled for this system.
Repository 'rhel-9-for-x86_64-baseos-rpms' is enabled for this system.
Repository 'rhel-9-for-x86_64-appstream-rpms' is enabled for this system.
[root@cephcluster03 ~]#
[root@kvmnode ~]#


[root@kvmnode ~]# ssh cephcluster01
Last login: Tue Mar 25 20:09:43 2025 from 192.168.29.23
[root@cephcluster01 ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sr0            11:0    1 1024M  0 rom
vda           252:0    0   60G  0 disk
├─vda1        252:1    0    1G  0 part /boot
└─vda2        252:2    0   59G  0 part
  ├─rhel-root 253:0    0 36.7G  0 lvm  /
  ├─rhel-swap 253:1    0  4.4G  0 lvm  [SWAP]
  └─rhel-home 253:2    0 17.9G  0 lvm  /home
vdb           252:16   0   40G  0 disk
vdc           252:32   0   40G  0 disk
vdd           252:48   0   40G  0 disk


[root@cephcluster02 ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sr0            11:0    1 1024M  0 rom
vda           252:0    0   60G  0 disk
├─vda1        252:1    0    1G  0 part /boot
└─vda2        252:2    0   59G  0 part
  ├─rhel-root 253:0    0 36.7G  0 lvm  /
  ├─rhel-swap 253:1    0  4.4G  0 lvm  [SWAP]
  └─rhel-home 253:2    0 17.9G  0 lvm  /home
vdb           252:16   0   40G  0 disk
vdc           252:32   0   40G  0 disk
vdd           252:48   0   40G  0 disk


[root@cephcluster03 ~]# lsblk
NAME          MAJ:MIN RM  SIZE RO TYPE MOUNTPOINTS
sr0            11:0    1 1024M  0 rom
vda           252:0    0   60G  0 disk
├─vda1        252:1    0    1G  0 part /boot
└─vda2        252:2    0   59G  0 part
  ├─rhel-root 253:0    0 36.7G  0 lvm  /
  ├─rhel-swap 253:1    0  4.4G  0 lvm  [SWAP]
  └─rhel-home 253:2    0 17.9G  0 lvm  /home
vdb           252:16   0   40G  0 disk
vdc           252:32   0   40G  0 disk
vdd           252:48   0   40G  0 disk
[root@cephcluster03 ~]#


[root@cephcluster01 ~]# podman login registry.redhat.io
Username: XXXXXX
Password:
Login Succeeded!
[root@cephcluster01 ~]#


[root@cephcluster02 ~]# podman login registry.redhat.io
Username: XXXXX
Password:
Login Succeeded!

[root@cephcluster03 ~]# podman login registry.redhat.io
Username: XXXXXX
Password:
Login Succeeded!


cephadm bootstrap --mon-ip  192.168.29.200 --skip-monitoring-stack

[root@cephcluster01 ~]#  podman login registry.redhat.io
Username: rrrvikas
Password:
Login Succeeded!
[root@cephcluster01 ~]# ip -br a
lo               UNKNOWN        127.0.0.1/8 ::1/128
enp1s0           UP             192.168.29.200/24 2405:201:4019:c115:5054:ff:feae:8cf/64 fe80::5054:ff:feae:8cf/64
[root@cephcluster01 ~]# cephadm bootstrap --mon-ip  192.168.29.200 --skip-monitoring-stack
Creating directory /etc/ceph for ceph.conf
Verifying podman|docker is present...
Verifying lvm2 is present...
Verifying time synchronization is in place...
Unit chronyd.service is enabled and running
Repeating the final host check...
podman (/usr/bin/podman) version 5.2.2 is present
systemctl is present
lvcreate is present
Unit chronyd.service is enabled and running
Host looks OK
Cluster fsid: 809c8354-0992-11f0-939c-525400ae08cf
Verifying IP 192.168.29.200 port 3300 ...
Verifying IP 192.168.29.200 port 6789 ...
Mon IP `192.168.29.200` is in CIDR network `192.168.29.0/24`
Mon IP `192.168.29.200` is in CIDR network `192.168.29.0/24`
Internal network (--cluster-network) has not been provided, OSD replication will default to the public_network
Pulling container image registry.redhat.io/rhceph/rhceph-5-rhel8:latest...
Ceph version: ceph version 16.2.10-275.el8cp (69b25b314048bb8542dce8512a249fc34a7add1d) pacific (stable)
Extracting ceph user uid/gid from container image...
Creating initial keys...
Creating initial monmap...
Creating mon...
firewalld ready
Enabling firewalld service ceph-mon in current zone...
Waiting for mon to start...
Waiting for mon...
mon is available
Assimilating anything we can from ceph.conf...
Generating new minimal ceph.conf...
Restarting the monitor...
Setting mon public_network to 192.168.29.0/24
Wrote config to /etc/ceph/ceph.conf
Wrote keyring to /etc/ceph/ceph.client.admin.keyring
Creating mgr...
Verifying port 9283 ...
firewalld ready
Enabling firewalld service ceph in current zone...
firewalld ready
Enabling firewalld port 9283/tcp in current zone...
Waiting for mgr to start...
Waiting for mgr...
mgr not available, waiting (1/15)...
mgr not available, waiting (2/15)...
mgr is available
Enabling cephadm module...
Waiting for the mgr to restart...
Waiting for mgr epoch 5...
mgr epoch 5 is available
Setting orchestrator backend to cephadm...
Generating ssh key...
Wrote public SSH key to /etc/ceph/ceph.pub
Adding key to root@localhost authorized_keys...
Adding host cephcluster01...
Deploying mon service with default placement...
Deploying mgr service with default placement...
Deploying crash service with default placement...
Enabling the dashboard module...
Waiting for the mgr to restart...
Waiting for mgr epoch 9...
mgr epoch 9 is available
Generating a dashboard self-signed certificate...
Creating initial admin user...
Fetching dashboard port number...
firewalld ready
Enabling firewalld port 8443/tcp in current zone...
Ceph Dashboard is now available at:

             URL: https://cephcluster01:8443/
            User: admin
        Password: ws42nnxn1m

Enabling client.admin keyring and conf on hosts with "admin" label
Enabling autotune for osd_memory_target
You can access the Ceph CLI as following in case of multi-cluster or non-default config:

        sudo /usr/sbin/cephadm shell --fsid 809c8354-0992-11f0-939c-525400ae08cf -c /etc/ceph/ceph.conf -k /etc/ceph/ceph.client.admin.keyring

Or, if you are only running a single cluster on this host:

        sudo /usr/sbin/cephadm shell

Please consider enabling telemetry to help improve Ceph:

        ceph telemetry on

For more information see:

        https://docs.ceph.com/en/pacific/mgr/telemetry/

Bootstrap complete.
[root@cephcluster01 ~]#
[root@cephcluster01 ~]# cephadm shell -- ceph -s
Inferring fsid 809c8354-0992-11f0-939c-525400ae08cf
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f
  cluster:
    id:     809c8354-0992-11f0-939c-525400ae08cf
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum cephcluster01 (age 6m)
    mgr: cephcluster01.yfnxny(active, since 6m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

[root@cephcluster01 ~]#

[root@cephcluster01 ~]# yum install -y ceph-common

cephadm shell -- ceph orch host add cephcluster02 192.168.29.201




[root@cephcluster01 ~]# cephadm shell -- ceph orch host add cephcluster02 192.168.29.201
Inferring fsid 809c8354-0992-11f0-939c-525400ae08cf
Using recent ceph image registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f
Error EINVAL: Failed to connect to cephcluster02 (192.168.29.201).
Please make sure that the host is reachable and accepts connections using the cephadm SSH key

To add the cephadm SSH key to the host:
> ceph cephadm get-pub-key > ~/ceph.pub
> ssh-copy-id -f -i ~/ceph.pub root@192.168.29.201

To check that the host is reachable open a new shell with the --no-hosts flag:
> cephadm shell --no-hosts

Then run the following:
> ceph cephadm get-ssh-config > ssh_config
> ceph config-key get mgr/cephadm/ssh_identity_key > ~/cephadm_private_key
> chmod 0600 ~/cephadm_private_key
> ssh -F ssh_config -i ~/cephadm_private_key root@192.168.29.201
[root@cephcluster01 ~]# ceph cephadm get-pub-key > ~/ceph.pub
[root@cephcluster01 ~]#  ssh-copy-id -f -i ~/ceph.pub root@192.168.29.201
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/root/ceph.pub"
The authenticity of host '192.168.29.201 (192.168.29.201)' can't be established.
ED25519 key fingerprint is SHA256:x4w1nu/H9g4/j7K2AzgVe7Zdlw1OWhivIgCEqmWeUnY.
This key is not known by any other names
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
root@192.168.29.201's password:

Number of key(s) added: 1

Now try logging into the machine, with:   "ssh 'root@192.168.29.201'"
and check to make sure that only the key(s) you wanted were added.

[root@cephcluster01 ~]# ceph orch host add cephcluster02 192.168.29.201
Added host 'cephcluster02' with addr '192.168.29.201'
[root@cephcluster01 ~]#

[root@cephcluster02 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster02 ~]# podman login registry.redhat.io
Username: rrrvikas
Password:
Error: logging into "registry.redhat.io": invalid username/password
[root@cephcluster02 ~]# podman login registry.redhat.io
Username: rrrvikas
Password:
Login Succeeded!
[root@cephcluster02 ~]# podman ps -a
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES

[root@cephcluster02 ~]# exit
logout
Connection to cephcluster02 closed.
[root@kvmnode ~]# ssh cephcluster01
Last login: Tue Mar 25 22:20:49 2025 from 192.168.29.23
[root@cephcluster01 ~]# ceph orch device ls
HOST           PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS
cephcluster01  /dev/vdb  hdd              42.9G  Yes        27m ago
cephcluster01  /dev/vdc  hdd              42.9G  Yes        27m ago
cephcluster01  /dev/vdd  hdd              42.9G  Yes        27m ago
[root@cephcluster01 ~]# ceph -s
  cluster:
    id:     809c8354-0992-11f0-939c-525400ae08cf
    health: HEALTH_WARN
            Failed to place 1 daemon(s)
            failed to probe daemons or devices
            OSD count 0 < osd_pool_default_size 3

  services:
    mon: 1 daemons, quorum cephcluster01 (age 58m)
    mgr: cephcluster01.yfnxny(active, since 58m)
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

[root@cephcluster01 ~]# ceph orch host add cephcluster02 192.168.29.201
Added host 'cephcluster02' with addr '192.168.29.201'
[root@cephcluster01 ~]# exit
logout
Connection to cephcluster01 closed.
[root@kvmnode ~]# ssh cephcluster02
Last login: Tue Mar 25 22:29:30 2025 from 192.168.29.23
[root@cephcluster02 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster02 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster02 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster02 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED         STATUS         PORTS                                                                                   NAMES
116b5e60eb83  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  58 seconds ago  Up 59 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster02
4447e2a6c7f3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mgr.cephcluste...  56 seconds ago  Up 57 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mgr-cephcluster02-yuqywp
d584bb66f4a8  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  53 seconds ago  Up 54 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster02
[root@cephcluster02 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED        STATUS        PORTS                                                                                   NAMES
116b5e60eb83  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  3 minutes ago  Up 3 minutes  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster02
4447e2a6c7f3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mgr.cephcluste...  3 minutes ago  Up 3 minutes  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mgr-cephcluster02-yuqywp
d584bb66f4a8  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  3 minutes ago  Up 3 minutes  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster02
[root@cephcluster02 ~]# exit
logout
Connection to cephcluster02 closed.
[root@kvmnode ~]# ssh cephcluster01
Last login: Tue Mar 25 22:31:50 2025 from 192.168.29.23
[root@cephcluster01 ~]# ceph -s
  cluster:
    id:     809c8354-0992-11f0-939c-525400ae08cf
    health: HEALTH_WARN
            OSD count 0 < osd_pool_default_size 3

  services:
    mon: 2 daemons, quorum cephcluster01,cephcluster02 (age 5m)
    mgr: cephcluster01.yfnxny(active, since 64m), standbys: cephcluster02.yuqywp
    osd: 0 osds: 0 up, 0 in

  data:
    pools:   0 pools, 0 pgs
    objects: 0 objects, 0 B
    usage:   0 B used, 0 B / 0 B avail
    pgs:

[root@cephcluster01 ~]# ceph orch ls
NAME   PORTS  RUNNING  REFRESHED  AGE  PLACEMENT
crash             2/2  4m ago     65m  *
mgr               2/2  4m ago     65m  count:2
mon               2/5  4m ago     65m  count:5
[root@cephcluster01 ~]# ceph orch ps
NAME                      HOST           PORTS   STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION            IMAGE ID      CONTAINER ID
crash.cephcluster01       cephcluster01          running (65m)     4m ago  65m    6627k        -  16.2.10-275.el8cp  d7a74ab527fa  3e2aecb4b12c
crash.cephcluster02       cephcluster02          running (5m)      4m ago   5m    6623k        -  16.2.10-275.el8cp  d7a74ab527fa  116b5e60eb83
mgr.cephcluster01.yfnxny  cephcluster01  *:9283  running (65m)     4m ago  65m     422M        -  16.2.10-275.el8cp  d7a74ab527fa  3424f62753bd
mgr.cephcluster02.yuqywp  cephcluster02  *:8443  running (5m)      4m ago   5m     386M        -  16.2.10-275.el8cp  d7a74ab527fa  4447e2a6c7f3
mon.cephcluster01         cephcluster01          running (65m)     4m ago  65m     200M    2048M  16.2.10-275.el8cp  d7a74ab527fa  c75a2561d7e8
mon.cephcluster02         cephcluster02          running (5m)      4m ago   5m    89.6M    2048M  16.2.10-275.el8cp  d7a74ab527fa  d584bb66f4a8
[root@cephcluster01 ~]# ceph orch device ls
HOST           PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS
cephcluster01  /dev/vdb  hdd              42.9G  Yes        4m ago
cephcluster01  /dev/vdc  hdd              42.9G  Yes        4m ago
cephcluster01  /dev/vdd  hdd              42.9G  Yes        4m ago
cephcluster02  /dev/vdb  hdd              42.9G  Yes        6m ago
cephcluster02  /dev/vdc  hdd              42.9G  Yes        6m ago
cephcluster02  /dev/vdd  hdd              42.9G  Yes        6m ago
[root@cephcluster01 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED            STATUS            PORTS                                                                                   NAMES
c75a2561d7e8  registry.redhat.io/rhceph/rhceph-5-rhel8:latest                                                                   -n mon.cephcluste...  About an hour ago  Up About an hour  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster01
3424f62753bd  registry.redhat.io/rhceph/rhceph-5-rhel8:latest                                                                   -n mgr.cephcluste...  About an hour ago  Up About an hour  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mgr-cephcluster01-yfnxny
3e2aecb4b12c  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  About an hour ago  Up About an hour  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster01
[root@cephcluster01 ~]# ceph orch device ls
HOST           PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS
cephcluster01  /dev/vdb  hdd              42.9G  Yes        6m ago
cephcluster01  /dev/vdc  hdd              42.9G  Yes        6m ago
cephcluster01  /dev/vdd  hdd              42.9G  Yes        6m ago
cephcluster02  /dev/vdb  hdd              42.9G  Yes        8m ago
cephcluster02  /dev/vdc  hdd              42.9G  Yes        8m ago
cephcluster02  /dev/vdd  hdd              42.9G  Yes        8m ago
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster01:/dev/vdb
Created osd(s) 0 on host 'cephcluster01'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster01:/dev/vdc
Created osd(s) 1 on host 'cephcluster01'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster01:/dev/vdd
^[Created osd(s) 2 on host 'cephcluster01'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster02:/dev/vdb
Created osd(s) 3 on host 'cephcluster02'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster02:/dev/vdc
Created osd(s) 4 on host 'cephcluster02'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster02:/dev/vdd
Created osd(s) 5 on host 'cephcluster02'
[root@cephcluster01 ~]#




ceph orch host add cephcluster03 192.168.29.202


\[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE       COMMAND     CREATED     STATUS      PORTS       NAMES
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED         STATUS         PORTS                                                                                   NAMES
2be283b0bb1d  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  35 seconds ago  Up 34 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster03
5fb950044cb3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  32 seconds ago  Up 32 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster03
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED         STATUS         PORTS                                                                                   NAMES
2be283b0bb1d  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  44 seconds ago  Up 44 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster03
5fb950044cb3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  41 seconds ago  Up 42 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster03
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED         STATUS         PORTS                                                                                   NAMES
2be283b0bb1d  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  51 seconds ago  Up 51 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster03
5fb950044cb3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  49 seconds ago  Up 49 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster03
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED         STATUS         PORTS                                                                                   NAMES
2be283b0bb1d  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  53 seconds ago  Up 52 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster03
5fb950044cb3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  50 seconds ago  Up 50 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster03
[root@cephcluster03 ~]# podman ps
CONTAINER ID  IMAGE                                                                                                             COMMAND               CREATED         STATUS         PORTS                                                                                   NAMES
2be283b0bb1d  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n client.crash.c...  54 seconds ago  Up 54 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-crash-cephcluster03
5fb950044cb3  registry.redhat.io/rhceph/rhceph-5-rhel8@sha256:85b8177ac80c1814108021b69463b27e6cdbe34df7bef4ef267474a2d1eaa94f  -n mon.cephcluste...  51 seconds ago  Up 52 seconds  80/tcp, 5000/tcp, 6789/tcp, 6800/tcp, 6801/tcp, 6802/tcp, 6803/tcp, 6804/tcp, 6805/tcp  ceph-809c8354-0992-11f0-939c-525400ae08cf-mon-cephcluster03
[root@cephcluster03 ~]# exit
logout
Connection to cephcluster03 closed.
[root@kvmnode ~]# ssh cephcluster0
ssh: Could not resolve hostname cephcluster0: Name or service not known
[root@kvmnode ~]# ssh cephcluster01
Last login: Tue Mar 25 22:57:15 2025 from 192.168.29.23
[root@cephcluster01 ~]# ceph orch device ls
HOST           PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS
cephcluster01  /dev/vdb  hdd              42.9G             16m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster01  /dev/vdc  hdd              42.9G             16m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster01  /dev/vdd  hdd              42.9G             16m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster02  /dev/vdb  hdd              42.9G             14m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster02  /dev/vdc  hdd              42.9G             14m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster02  /dev/vdd  hdd              42.9G             14m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster03  /dev/vdb  hdd              42.9G  Yes        69s ago
cephcluster03  /dev/vdc  hdd              42.9G  Yes        69s ago
cephcluster03  /dev/vdd  hdd              42.9G  Yes        69s ago
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster03:/dev/vdb
Created osd(s) 6 on host 'cephcluster03'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster03:/dev/vdc
Created osd(s) 7 on host 'cephcluster03'
[root@cephcluster01 ~]# ceph orch daemon add osd cephcluster03:/dev/vdd
Created osd(s) 8 on host 'cephcluster03'
[root@cephcluster01 ~]# ceph orch device ls
HOST           PATH      TYPE  DEVICE ID   SIZE  AVAILABLE  REFRESHED  REJECT REASONS
cephcluster01  /dev/vdb  hdd              42.9G             17m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster01  /dev/vdc  hdd              42.9G             17m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster01  /dev/vdd  hdd              42.9G             17m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster02  /dev/vdb  hdd              42.9G             16m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster02  /dev/vdc  hdd              42.9G             16m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster02  /dev/vdd  hdd              42.9G             16m ago    Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster03  /dev/vdb  hdd              42.9G             2s ago     Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster03  /dev/vdc  hdd              42.9G             2s ago     Insufficient space (<10 extents) on vgs, LVM detected, locked
cephcluster03  /dev/vdd  hdd              42.9G             2s ago     Insufficient space (<10 extents) on vgs, LVM detected, locked
[root@cephcluster01 ~]# ceph -s
  cluster:
    id:     809c8354-0992-11f0-939c-525400ae08cf
    health: HEALTH_OK

  services:
    mon: 3 daemons, quorum cephcluster01,cephcluster02,cephcluster03 (age 2m)
    mgr: cephcluster01.yfnxny(active, since 86m), standbys: cephcluster02.yuqywp
    osd: 9 osds: 9 up (since 13s), 9 in (since 23s)

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   52 MiB used, 360 GiB / 360 GiB avail
    pgs:     1 active+clean

[root@cephcluster01 ~]#
[root@cephcluster01 ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME               STATUS  REWEIGHT  PRI-AFF
-1         0.35184  root default
-3         0.11728      host cephcluster01
 0    hdd  0.03909          osd.0               up   1.00000  1.00000
 1    hdd  0.03909          osd.1               up   1.00000  1.00000
 2    hdd  0.03909          osd.2               up   1.00000  1.00000
-5         0.11728      host cephcluster02
 3    hdd  0.03909          osd.3               up   1.00000  1.00000
 4    hdd  0.03909          osd.4               up   1.00000  1.00000
 5    hdd  0.03909          osd.5               up   1.00000  1.00000
-7         0.11728      host cephcluster03
 6    hdd  0.03909          osd.6               up   1.00000  1.00000
 7    hdd  0.03909          osd.7               up   1.00000  1.00000
 8    hdd  0.03909          osd.8               up   1.00000  1.00000
[root@cephcluster01 ~]#




